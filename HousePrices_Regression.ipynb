{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGx8ccCnHI3l8XNBPZV7J2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdmarghe/DeepLearning/blob/main/HousePrices_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATI"
      ],
      "metadata": {
        "id": "zQ9HoXM4jWyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boston House prices analisi\n",
        "Regressione lineare continua (da non confondere con algoritmo di regressione logistica che risolve un problema di classificazione)\n"
      ],
      "metadata": {
        "id": "1arzOGfVjFoZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1bjk_HIHFWW",
        "outputId": "e637fe5e-bcd6-442e-c8d5-daf090c0fe6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le varie caratteristiche dei dati di input hanno scale diverse--> Vogliamo normalizzare i dati affinchè ogni caratteristica sia su una stessa scala di valori"
      ],
      "metadata": {
        "id": "V1W6aUKxjfgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAwUlxwcjdBT",
        "outputId": "5a7562de-a5ce-4dfc-cffb-cdb6c877db5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohPwrlVKjx9U",
        "outputId": "8a4dd385-3d6b-4077-bcf7-766425b3fb7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abbiamo veramente pochi dati!"
      ],
      "metadata": {
        "id": "3ltHzWNdj1Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets[0] #la prima casa del dataframe costa 15,2k dollari"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5TkC46cjzwL",
        "outputId": "5dbe03c6-820e-49ae-b4e5-f15faefcb3d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(15.2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparazione Dati\n",
        "###Normalizzazione delle caratteristiche dei dati: ogni colonna verrà sottratta del vettore media e divisa per il valore deviazione standard. Con questo metodo avremo una distribuzione centrata in 0 e con deviazione standard unitaria."
      ],
      "metadata": {
        "id": "cL5F0W7Kkanc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean=train_data.mean(axis=0)\n",
        "train_data-=mean\n",
        "std=train_data.std(axis=0)\n",
        "train_data/=std\n",
        "test_data-=mean\n",
        "test_data/=std\n"
      ],
      "metadata": {
        "id": "o3lMq_rjj8ik"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKZ2awEtlBxR",
        "outputId": "de3dedf7-2356-46e3-9a20-cd5dba0b6ec5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
              "       -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
              "        1.14850044,  0.44807713,  0.8252202 ])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creazione della rete\n",
        "Siccome la rete ha a disposizione pochissimi dati sarà sottoposta maggiormente all'overfitting.\n",
        "Utilizzeremo dunque il metodo delle K-fold per convalidare la nostra rete: Suddividiamo il campione di addestramento in k gruppi e iteriamo l'addestramento della rete k volte, ogni volta con k-1 gruppi di addestramento e uno di convalida. Questo ci permette di capire di quante epoche potremmo avere bisogno in maniera omogenea.\n"
      ],
      "metadata": {
        "id": "JWlrqo2wlSFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers, models"
      ],
      "metadata": {
        "id": "wQHr_93qlPN4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funzione di attivazione, funzione obiettivo e metrica\n",
        "L'ultimo layer della rete non deve avere attivazione perchè deve essere un layer lineare e una attivazione (es una sigmoide) ci limiterebbe l'intervallo di predizione.\n",
        "La funzione obiettivo mse, mean squared error utilizzata per regressioni.\n",
        "La metrica Mae, mean absolute error, ci dice di quanto si scosta il prezzo dalla media."
      ],
      "metadata": {
        "id": "F1dMqSB7miGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  model=models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "Sk1rD2rFmQTB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convalida approccio\n",
        "The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions (typically K = 4 or 5), instantiating K identical models, and training each one on K – 1 partitions while evaluating on the remaining partition. The validation score for the model used is then the average of the K validation scores obtained."
      ],
      "metadata": {
        "id": "ao4oic5-n2gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    #1\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    partial_train_data = np.concatenate(                                     #2\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    model = build_model()                                                    #3\n",
        "    model.fit(partial_train_data, partial_train_targets,                     #4\n",
        "              epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)      #5\n",
        "    all_scores.append(val_mae)\n",
        "\n",
        "#1 - Prepares the validation data: data from partition #k\n",
        "#2 - Prepares the training data: data from all other partitions\n",
        "#3 - Builds the Keras model (already compiled)\n",
        "#4 - Trains the model (in silent mode, verbose = 0)\n",
        "#5 - Evaluates the model on the validation data\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNxDyn8Sn4y_",
        "outputId": "696b4c1f-9902-4590-bb30-93ae30219499"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_-Pr48RoRPx",
        "outputId": "f0dcabfb-8678-4cf2-aea3-e2ec6f0d9387"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.0039353370666504, 2.5561344623565674, 2.94840145111084, 2.598262310028076]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(all_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTPgHZ8UoSYC",
        "outputId": "04e622af-2f91-4cbf-8daa-c9628a367eab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(2.5266833901405334)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Retrain a 500 epoche"
      ],
      "metadata": {
        "id": "Y2WshYLwpmW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    #1\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate(                                     #2\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    model = build_model()                                                    #3\n",
        "    history = model.fit(partial_train_data, partial_train_targets,           #4\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    mae_history = history.history['val_maer']\n",
        "    all_mae_histories.append(mae_history)\n",
        "\n",
        "#1 - Prepares the validation data: data from partition #k\n",
        "#2 - Prepares the training data: data from all other partitions\n",
        "#3 - Builds the Keras model (already compiled)\n",
        "#4 - Trains the model (in silent mode, verbose=0)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY7jYMUNoaSO",
        "outputId": "4ce4d307-764c-4c81-cc97-7a3d20017c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in history.history.keys():\n",
        "  print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUW14uMrsLBN",
        "outputId": "460747ea-5eb3-4a4b-f054-750ae888942e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss\n",
            "mae\n",
            "val_loss\n",
            "val_mae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avarage_mae_history=[np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
      ],
      "metadata": {
        "id": "IZgkwUJWp4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r6iNa8CNqhAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}